\input{header_beamer}

\usecolortheme{default}
% colours
\xdefinecolor{Black}{rgb}{0,0,0}
\xdefinecolor{White}{rgb}{1,1,1}
\xdefinecolor{DarkBlue}{rgb}{0,0,.7}
\xdefinecolor{DarkRed}{rgb}{.7,0,0}
\xdefinecolor{Red}{rgb}{.85,0,0}
\xdefinecolor{DarkGreen}{rgb}{0,.7,0}
\xdefinecolor{DarkMagenta}{rgb}{.6,0,.6}
\def\Black{\textcolor{Black}}
\def\White{\textcolor{White}}
\def\Blue{\textcolor{DarkBlue}}
\def\Magenta{\textcolor{DarkMagenta}}
\def\Red{\textcolor{Red}}
\def\Green{\textcolor{DarkGreen}}

\usepackage{alltt}
\usepackage{psfrag}
\usepackage{pstool}

\title[] % (optional, use only with long paper titles)
{Introduction to probabilistic programming}

\author % (optional, use only with lots of authors)
{David Duvenaud and James Lloyd}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[] % (optional, but mostly needed)
{University of Cambridge}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date % (optional)
{\empty}

\subject{Talks}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes}
 \makeatletter
 \tikzset{join/.code=\tikzset{after node path={%
       \ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
       edge[every join]#1(\tikzchaincurrent)\fi}}
 }
 \tikzset{>=stealth',every on chain/.append style={join},
   every join/.style={->}
 }

\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{ifthen}
\usepackage{booktabs}

% Custom definitions
\def\simiid{\sim_{\mbox{\tiny iid}}}

\input{commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\begin{document}

\small

%% { 
%%   \setbeamertemplate{footline}{\empty}
%%   \begin{frame}
%%     \titlepage
%%   \end{frame}
%% }
\renewcommand{\inserttotalframenumber}{11}

\input{defs}

\begin{frame}
  \begin{block}{}
    \titlepage
  \end{block}
  \begin{center}
    {\bf Thanks to}\\
    Daniel M Roy (Cambridge)\\
    Roger Grosse (MIT)
  \end{center}
\end{frame}

\begin{frame}{How to write a Bayesian modeling paper}
  \begin{block}{}
    \begin{enumerate}
      \item Write down a generative model in an afternoon
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Get 2 grad students to implement inference for a month
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Use technical details of inference to pad half of the paper
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}{Can we do better?}
  \begin{block}{Example: Graphical Models}  
  \end{block}
      \begin{block}{Application Papers}
      \begin{enumerate}
        \item Write down a graphical model
        \item Perform inference using general-purpose software
        \item Apply to some new problem
      \end{enumerate}
    \end{block}
      \begin{block}{Inference papers}
      \begin{enumerate}
        \item Identify common structures in graphical models (e.g. chains)
        \item Develop efficient inference method
        \item Implement in a general-purpose software package
      \end{enumerate}
    \end{block}  
  \begin{block}{}
      \vspace{-2\baselineskip}
  \large
    \begin{center}
    {
      {Modeling and inference have been disentangled}
    }	
    \end{center}
  \end{block}
\end{frame}



\begin{frame}{Expressivity}
  \begin{block}{Not all models are graphical models}  
  What is the largest class of models available?
  \end{block}
      \begin{block}{Probabilistic Programs}
      \begin{enumerate}
        \item A probabilistic program (PP) is any program that can depend on random choices.  Can be written in any language that has a (P)RNG.
        \item You can specify any (computable) prior by simply writing down a PP that generates samples
        \item Any PP implicitly defines a distribution over execution traces
      \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{An Example Probabilistic Program}
 % \begin{block}{}
    \begin{alltt}
    function x = simplemix()
    
    \
    
	flip = rand < 0.5;

	if flip

	\ \ x = randg + 2;   \% Random draw from Gamma(1,1)
    
	else

	\ \ x = randn;    \ \ \ \ \% Random draw from standard Normal
    
	end

    \end{alltt}
%  \end{block}
  \TBD{Call attention to inelegance (and perhaps lack of generic inference schemes) as a graphical model}
  \TBD{Maybe add a unboundedly recursive example as well}
	\pause
  \begin{block}{Implied distributions over variables}

  %\begin{figure}
  \only<1-2>{
	  \includegraphics[width=0.45\textwidth]{figures/xmarg}
	  \includegraphics[width=0.45\textwidth]{figures/flipmarg}}
  \only<3>{
	  \includegraphics[width=0.45\textwidth]{figures/joint}
	  \includegraphics[width=0.45\textwidth]{figures/flipcond}}	  
%	  \includegraphics[width=0.4\textwidth]{figures/joint}
	 % \end{figure}
  \end{block}
\end{frame}

\begin{frame}{Probabilistic Programs vs Probabilistic Programming}
\TBD{Move this between earlier slides}
  \begin{block}{Once we've defined a prior, what do we want to do with it?}  
  The PP defines $P(D,N,H)$, we choose D to be the subset of variables we observe, H the set of variables we're interested in, and N the set of variables that we're not interested in, so we'll integrate them out.  We want to get to $P(H|D)$
  \end{block}
      \begin{block}{Probabilistic Programming}
      \begin{enumerate}
        \item Usually refers to doing inference when a PP specifies your prior.
        \item 
      \end{enumerate}
    \end{block}
    TODO: Show the two possibilities of conditioning in th eprevious program
\end{frame}

\begin{frame}{Can we develop generic inference for all PPs?}
Motivation: Split model specification (that is very expressive) and inference

Yes - rejection sampling. Draw \eg 3 traces and their evaluations. Show which ones match a condition.

This produces a sample over \emph{random choices} in the \emph{execution trace}.
\end{frame}

\begin{frame}{Can we be more efficient}
MCMC in words.

Start with a trace. (Picture)

Propose a new trace by modifying one random choice (Picture)

and sampling subsequent choices changed/altered by modification (Picture)

Accept proposal using approriate RJ-MCMC acceptance probability

\TBD{Work out where to mention the conditioning}
\end{frame}

\begin{frame}{Math}
Same thing but replace pictures with math
\end{frame}

\begin{frame}{PP via MCMC}
  \begin{block}{}
    Following Wingate et alia we represent the unconditioned PP as a parameterless function $f$
    \newline
    
    Evaluating $f$ results in random choices which are denoted as
    \begin{equation*}
      x_k = f_{k|x_1,\ldots,x_{k-1}} \sim p_{t_k}(.|\theta_{k},x_1,\ldots,x_{k-1}).
    \end{equation*}
    
    The density / probability of a particular evaluation is then
    \begin{equation*}
      p(x) = \prod_{k=1}^K p_{t_k}(x_k|\theta_{k},x_1,\ldots,x_{k-1}).
    \end{equation*}
    
    We then perform MCMC over the $x_k$ \ie the execution trace.
    
  \end{block}
\end{frame}

\begin{frame}{MCMC over execution traces}
  \begin{enumerate}
    \item Select a random $x_k = f_k$ in the execution trace
    \item Propose a new value $x_k' \sim K_{t_k}(.|x_k,\theta_k)$
    \item Run the program to determine all subsequent choices $(x_l' : l > k)$, reusing current choices where possible
    \item Propose moving from the state $(x_1,\ldots,x_K)$ to $(x_1,\ldots,x_{k-1},x_k',\ldots,x_{K'}')$
    \item Accept the change with the appropriate reversible jump MCMC acceptance probability, this includes terms like
    \begin{enumerate}
      \item $K_{t_k}(x_k'|x_k,\theta_k),\,K_{t_k}(x_k|x_k',\theta_k),\,p_{t_k}(x_k|\theta_{k},x_1,\ldots,x_{k-1})$
      \item $\prod_{i=k}^K p_{t_i}(x_i|\theta_{i},x_1,\ldots,x_{i-1}),\,\prod_{i=k}^{K'} p_{t_i'}(x_i'|\theta_{i}',x_1,\ldots,x_{k-1},x_k',\ldots,x_{i-1}')$
      \item \ie $\frac{K_{t_k}(x_k|x_k',\theta_k)\prod_{i=k}^{K'} p_{t_i'}(x_i'|\theta_{i}',x_1,\ldots,x_{k-1},x_k',\ldots,x_{i-1}')}{K_{t_k}(x_k'|x_k,\theta_k)\prod_{i=k}^K p_{t_i}(x_i|\theta_{i},x_1,\ldots,x_{i-1})}$
    \end{enumerate}
  \end{enumerate}
\end{frame}

\begin{frame}{Worked example}
\TBD{If there is time}
\end{frame}

\begin{frame}{Further generic inference methods}
\TBD{Dave}
Now that we have separated inference and model design, what inference methods could we do

\eg HMC, parallel tempering, etc. (with citations)

Remember graphical models (fancy algorithms that work in certain model classes)
\end{frame}

\begin{frame}{Main strength}
\TBD{Dave}
Writing non-parametric models easy

Only need to evaluate samples from non-parametric objext lazily
\end{frame}

\begin{frame}{Nonparametric example}
If we can sample from the prior of a nonparametric model using finite resources (with probability 1), then we can apply probabilistic programming.
\newline

We can do this simply for a number of NPB models. In particular, we might be interested in models that use Dirichlet processes. We give an example in MATLAB
\newline

Point is that even if one can sample from posterior finitely, still might not be able to do PP. (PP requires weaker(?) condition that we can sample from prior finitely). Active research to produce finite algorithms to sample from nonparametric objects \eg Dan
\end{frame}

\begin{frame}{Example: Mixture of Gaussians}
\TBD{Rewrite - create the sampling function $\Theta$ explicitly}
  \begin{columns}
    \begin{column}{.5\textwidth}
      \begin{block}{Generative model}
        \begin{eqnarray*}
          (\mu_i)_{i=1\ldots k} & \simiid & \mathcal{N}(0, 1) \\
          (\pi_i)_{i=1\ldots k} & \sim & \textrm{Dir}(\alpha) \\
          \Theta & := & \sum_{i=1}^k \pi_i \delta_{\mu_i} \\
          (\theta_i)_{i=1\ldots n} & \simiid & \Theta \\
          (x_i)_{i=1\ldots n} & \simiid & \mathcal{N}(\theta_i, 1)
        \end{eqnarray*}
      \end{block}
    \end{column}
    \begin{column}{.5\textwidth}
      \begin{block}{(Pseudo) MATLAB code}
        \vspace{0.75\baselineskip}
        \begin{alltt}
          mu = randn(k,1);

          pi = dirichlet(k, alpha);


          for i = 1:n
            
          \ \ theta = mu(mnrnd(1,pi));
          
          \ \ x(i) \ = theta + randn;
            
          end
        \end{alltt}
        \vspace{0.75\baselineskip}
      \end{block}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Stolen stick breaking slide}
\end{frame}

\begin{frame}{Example: Infinite mixture of Gaussians}
\TBD{Make this match the style of the previous page}
  \begin{block}{Change to generative model}
    \begin{equation*}
      \Theta := \sum_{i=1}^k \pi_i \delta_{\mu_i} \to \Theta \sim \textrm{DP}(\alpha, \mathcal{N}(0,1))
    \end{equation*}
  \end{block}
  \begin{block}{(Pseudo) MATLAB code - stick breaking construction}
    \begin{alltt}
      sticks = []; atoms = [];
      
      for i = 1:n
      
      \ \ p = rand;
      
      \ \ while p > sum(sticks)
      
      \ \ \ \ sticks(end+1) = (1-sum(sticks)) * betarnd(1, alpha);
      
      \ \ \ \ atoms(end+1) \ = randn;
      
      \ \ end
      
      \ \ theta(i) = atoms(find(cumsum(sticks)>=p, 1, 'first'));
      
      end

      x = theta' + randn(n, 1);
    \end{alltt}
  \end{block}
\end{frame}

\begin{frame}{PP timeline / videos / whatever}

Infer.net?
\end{frame}

\end{document}


