\input{header_beamer}
\input{matlab_setup}

\usecolortheme{default}
% colours
\xdefinecolor{Black}{rgb}{0,0,0}
\xdefinecolor{White}{rgb}{1,1,1}
\xdefinecolor{DarkBlue}{rgb}{0,0,.7}
\xdefinecolor{DarkRed}{rgb}{.7,0,0}
\xdefinecolor{Red}{rgb}{.85,0,0}
\xdefinecolor{DarkGreen}{rgb}{0,.7,0}
\xdefinecolor{DarkMagenta}{rgb}{.6,0,.6}
\def\Black{\textcolor{Black}}
\def\White{\textcolor{White}}
\def\Blue{\textcolor{DarkBlue}}
\def\Magenta{\textcolor{DarkMagenta}}
\def\Red{\textcolor{Red}}
\def\Green{\textcolor{DarkGreen}}

\usepackage{alltt}
\usepackage{psfrag}
\usepackage{pstool}
\usepackage{ulem}
%\usepackage{listings}

\usepackage{picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]

\title[] % (optional, use only with long paper titles)
{Introduction to probabilistic programming}

\author % (optional, use only with lots of authors)
{David Duvenaud and James Lloyd}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[] % (optional, but mostly needed)
{University of Cambridge}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date % (optional)
{\empty}

\subject{Talks}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes}
 \makeatletter
 \tikzset{join/.code=\tikzset{after node path={%
       \ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
       edge[every join]#1(\tikzchaincurrent)\fi}}
 }
 \tikzset{>=stealth',every on chain/.append style={join},
   every join/.style={->}
 }

\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{ifthen}
\usepackage{booktabs}

% Custom definitions
%\def\simiid{\sim_{\mbox{\tiny iid}}}
\def\simiid{\stackrel{\mathrm{iid}}{\sim}}

\input{commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\begin{document}

\small

%% { 
%%   \setbeamertemplate{footline}{\empty}
%%   \begin{frame}
%%     \titlepage
%%   \end{frame}
%% }
\renewcommand{\inserttotalframenumber}{11}

\input{defs}

\begin{frame}
  \begin{block}{}
    \titlepage
  \end{block}
  \begin{center}
    {\bf Thanks to}\\
    Daniel M Roy (Cambridge)\\
    Roger Grosse (MIT)
  \end{center}
\end{frame}

\begin{frame}{How to write a Bayesian modeling paper}
  \begin{block}{}
    \begin{enumerate}
      \item Write down a generative model in an afternoon
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Get 2 grad students to implement inference for a month
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Use technical details of inference to pad half of the paper
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}{Can we do better?}
  \begin{block}{Example: Graphical Models}  
  \end{block}
      \begin{block}{Application Papers}
      \begin{enumerate}
        \item Write down a graphical model
        \item Perform inference using general-purpose software
        \item Apply to some new problem
      \end{enumerate}
    \end{block}
      \begin{block}{Inference papers}
      \begin{enumerate}
        \item Identify common structures in graphical models (e.g. chains)
        \item Develop efficient inference method
        \item Implement in a general-purpose software package
      \end{enumerate}
    \end{block}  
  \begin{block}{}
      \vspace{-2\baselineskip}
  \large
    \begin{center}
    {
      {Modeling and inference have been disentangled}
    }	
    \end{center}
  \end{block}
\end{frame}



\begin{frame}{Expressivity}
  \begin{block}{Not all models are graphical models}  
  What is the largest class of models available?
  \end{block}
  \vspace{\baselineskip}
  \begin{block}{Probabilistic Programs}
      \begin{itemize}
        \item A probabilistic program (PP) is any program that can depend on random choices.  Can be written in any language that has a random number generator.
        \item You can specify any (computable) prior by simply writing down a PP that generates samples
        \item Any PP implicitly defines a distribution over its output.
        \begin{itemize}
		     \item It also defines a distribution over \emph{execution traces}: the set of states the interpreter went through when running the program.
        \end{itemize}
      \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{An Example Probabilistic Program}
\input{include/simple-mix-comments.tex}
\pause
  \begin{block}{Implied distributions over variables}

  %\begin{figure}
  \only<1-2>{
	  \includegraphics[width=0.45\textwidth]{figures/xmarg}
	  \includegraphics[width=0.45\textwidth]{figures/flipmarg}}
  \only<3>{
	  \includegraphics[width=0.45\textwidth]{figures/joint}
	  \includegraphics[width=0.45\textwidth]{figures/flipmarg}}
  \end{block}
\end{frame}

\begin{frame}{Probabilistic Programming: Conditioning}
%\TBD{DD}
%\TBD{Move this between earlier slides if sensible}
  \begin{block}{Once we've defined a prior, what can we do with it?}  

  The stochastic program defines joint distribution $P(D,N,H)$
  \begin{itemize}
    \item D to be the subset of variables we observe (condition on)
    \item H the set of variables we're interested in
    \item N the set of variables that we're not interested in, (so we'll integrate them out).
  \end{itemize}
  We want to know about $P(H|D)$
  \end{block}
  \begin{block}{Probabilistic Programming}
      \begin{itemize}
        \item Usually refers to doing conditionial inference when a probabilistic program specifies your prior.
      \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{An Example Probabilistic Program: Conditioning}
\input{include/simple-mix-comments.tex}
  \begin{block}{Implied distributions over variables}

  %\begin{figure}
  \only<1>{
	  \includegraphics[width=0.45\textwidth]{figures/joint}
	  \includegraphics[width=0.45\textwidth]{figures/flipmarg}}
  \only<2>{
	  \includegraphics[width=0.45\textwidth]{figures/joint}
	  \includegraphics[width=0.45\textwidth]{figures/flipcond}}
  \end{block}
\end{frame}

\begin{frame}{Can we develop generic inference for all PPs?}
  \begin{block}{Rejection sampling}
    \begin{enumerate}
      \item Run the program with a fresh source of random numbers
      \item If condition $D$ is true, record $H$ as a sample, else ignore the sample
      \item Repeat
    \end{enumerate}
  \end{block}
  \begin{block}{Example}
    %\vspace{0.5\baselineskip}
    %\centering
    %\input{figures/rejection-trace.tex}
    \begin{columns}
      \begin{column}{.4\textwidth}
        \input{include/simple-mix.tex}
      \end{column}
      \begin{column}{.4\textwidth}
        \input{include/rejection-trace.tex}
      \end{column}
    \end{columns}
  \end{block}
  \begin{block}{This produces samples over the \emph{execution trace}}
    \eg 
    \onslide<3->{\texttt{(True, 2.7)},}
    \onslide<9->{\texttt{(True, 2.1)},}
    \onslide<15->{\texttt{(False, 2.3)}, \dots}
  \end{block}
\end{frame}

\begin{frame}{Can we be more efficient?}
  \begin{block}{Metropolis-Hastings}
    \input{include/mcmc-trace.tex}
  \end{block}
\end{frame}

\begin{frame}{Can we be more efficient?}
  \begin{block}{Metropolis-Hastings}
    \input{include/mcmc-trace-2.tex}
  \end{block}
\end{frame}

\begin{frame}{PP via Metropolis-Hastings - notation}
  \begin{block}{}
    %Following Wingate et alia \TBD{ref} we represent an unconditioned PP as a parameterless random function $f$ (or as a deterministic function which takes a source of randomness as an input)
    %\newline
    %
    Evaluating a program results in a sequence of random choices
    \begin{equation*}
      x_k \sim p_{t_k}(.|\theta_{k},x_1,\ldots,x_{k-1}).
    \end{equation*}
    where $\theta_{k}$ represents parameters of the distribution not included in $(x_1,\ldots,x_{k-1})$.
    \newline
    
    The density / probability of a particular evaluation is then
    \begin{equation*}
      p(x_1,\ldots,x_K) = \prod_{k=1}^K p_{t_k}(x_k|\theta_{k},x_1,\ldots,x_{k-1}).
    \end{equation*}
    
    We then perform MH over the the execution trace $x = (x_1,\ldots,x_K)$.
    
  \end{block}
\end{frame}

\begin{frame}{MH over execution traces}
  \begin{enumerate}
    \item Select a random decision in the execution trace $x$
    \begin{itemize}
      \item{\eg $x_k$}
    \end{itemize}
    \vspace{\baselineskip}
    \item Propose a new value
    \begin{itemize}
      \item{\eg $x_k' \sim K_{t_k}(.|x_k,\theta_k)$}
    \end{itemize}
    \vspace{\baselineskip}
    \item Run the program to determine all subsequent choices $(x_l' : l > k)$, reusing current choices where possible
    \vspace{\baselineskip}
    \item Propose moving from the state $(x_1,\ldots,x_K)$ to $(x_1,\ldots,x_{k-1},x_k',\ldots,x_{K'}')$
    \vspace{\baselineskip}
    \item Accept the change with the appropriate MH acceptance probability
  \end{enumerate}
  \begin{equation*}
    \frac{K_{t_k}(x_k|x_k',\theta_k)\prod_{i=k}^{K'} p_{t_i'}(x_i'|\theta_{i}',x_1,\ldots,x_{k-1},x_k',\ldots,x_{i-1}')}{K_{t_k}(x_k'|x_k,\theta_k)\prod_{i=k}^K p_{t_i}(x_i|\theta_{i},x_1,\ldots,x_{i-1})}
  \end{equation*}
\end{frame}

%\begin{frame}{Worked example}
%\TBD{If there is time}
%\end{frame}

\begin{frame}{Advanced Automatic Inference}

    \begin{itemize}
      \item Now that we have separated inference and model design, can use any inference algorithm.
      \item Free to develop inference algorithms independently of specific models.
      \item Once graphical models identified as a general class, many model-agnostic inference methods:
      \begin{itemize}
	      \item Belief Propagation
	      \item Pseudo-likelihood
	      \item Mean-field Variational
	      \item MCMC
	    \end{itemize}
    \item What generic inference algorithms can we implement for more expressive generative models?
    \end{itemize}
\end{frame}


\begin{frame}{Advanced Automatic Inference: Gibbs}
    \begin{itemize}
      \item BUGS: {\bf B}ayesian inference {\bf U}sing {\bf G}ibbs {\bf S}ampling
        \begin{itemize}
      \item An early, limited form of automated inference in generative models.
      \item Began in 1989 in the MRC Biostatistics Unit, Cambridge, UK.
      \item A workhorse of applied statisticians.  Also JAGS (open-source)
    \end{itemize}
    \end{itemize}
    
\begin{alltt}
{\tiny
\input{include/bugs-example.txt.tex}}
\end{alltt}    

\end{frame}

%\begin{frame}{Advanced Automatic Inference: Exact}
%    \begin{itemize}
%      \item Cosh (exhaustive enumeration)
%      \item Stuhlmueller's recent work
%      \item Wingate's enumeration
%    \end{itemize}
%\end{frame}

\begin{frame}{Advanced Automatic Inference: Metropolis-Hastings}
    \begin{itemize}
      \item Bher, MIT-Church
      \\ {\color{DarkBlue}(Goodman, Mansinghka, Roy, Bonawitz and Tenenbaum, 2008)}
      \begin{itemize}
         \item (Automatic inference in Scheme)
      \end{itemize}      
      \item Stochastic Matlab
    \begin{itemize}
      \item Lightweight Implementations of Probabilistic Programming Languages Via Transformational Compilation {\color{DarkBlue} 
      
      (Wingate, Stuhlm\"{u}ller, Goodman, 2011)}
    \end{itemize}      
            \includegraphics[width=5.5cm]{figures/probmatlab.png}
            \includegraphics[width=5.5cm]{figures/probmatlab2.png}
    \end{itemize}
\end{frame}

\begin{frame}{Advanced Automatic Inference: HMC}

    \begin{itemize}
      \item Automatic Differentiation in Church:
      \\ Nonstandard Interpretations of Probabilistic Programs for Efficient Inference
{\color{DarkBlue} (Wingate, Goodman, Stuhlmuller, Siskind, 2012)}
      \item Stan (Gelman et al)
      \\
      \texttt{http://mc-stan.org/}
      \begin{alltt}
{\tiny
\input{include/stan-example.tex}}
\end{alltt}  
      
    \end{itemize}

\end{frame}

\begin{frame}{Advanced Automatic Inference: Expectation Propagation}
    \begin{itemize}
      \item Infer.NET {\color{DarkBlue} (Minka, Winn, Guiver, Knowles, 2012)}
      \begin{itemize}
        \item EP in graphical models:
        \item Now works in functional language F\#:
        \includegraphics[width=8cm]{figures/fun}
      \end{itemize}
    \end{itemize}
    
\end{frame}

\begin{frame}{Advanced Automatic Inference: Variational}

    \begin{itemize}
      \item Infer.NET has it too.
      \item Automated Variational Inference in Probabilistic Programming
      \\ {\color{DarkBlue}  (Wingate, Weber, 2012) }
      \\\includegraphics[width=8cm]{figures/meanvar}
      \item Learning phase: Forward sample, then stochastically update $\theta$s to minimize expected KL from true distribution.
      \item Dependency of variatonal dist on control logic remains.
    \end{itemize}

\end{frame}


\begin{frame}{Advanced Automatic Inference: Hardware}
    \begin{itemize}
      \item Natively Probabilistic Computation {\color{DarkBlue}  (Mansinghka, 2009) }
      \item Lyric Semiconductor? (Error correcting codes)
      \item Main idea: If we know we're going to be sampling, might as well take advantage of thermal noise in our hardware.
          \begin{itemize}
                \item Samplers can be made robust to computational error.
                \item Run at low voltage on (cheap?) FPGA
          \end{itemize}        
      \item Compile from generative model to FPGA (9x9 Ising model sampler):
      \\\includegraphics[width=5cm]{figures/llvm}
    \end{itemize}
\end{frame}

%\begin{frame}{Main strength}
%\TBD{Dave}
%Writing non-parametric models easy

%Only need to evaluate samples from non-parametric objext lazily
%\end{frame}

\begin{frame}{Nonparametric models}
  \begin{itemize}
    \item If we can sample from the prior of a nonparametric model using finite resources with probability 1, then we can apply probabilistic programming
    \vspace{\baselineskip}
    \item This can be achieved for a number of nonparametric processes/models \eg
    \begin{itemize}
      \item Marginalisation (\eg Gaussian process)
      \item Stick breaking (\eg Dirichlet process)
      \item Urn scheme (\eg Indian Buffet process)\dots
    \end{itemize}
    \vspace{\baselineskip}
    \item Active research to produce finite sampling algorithms for other nonparametric processes (\eg hierarchical beta processes, negative binomial process \TBD{cite Koa and Dan})
  \end{itemize}
\end{frame}



\begin{frame}{Example: Mixture of Gaussians}
  \begin{columns}
    \begin{column}{.47\textwidth}
      \begin{block}{Generative model}
        \begin{eqnarray*}
          (\mu_k)_{k=1\ldots K} & \simiid & \mathcal{N}(0, 1) \\
          (\pi_k)_{k=1\ldots K} & \sim & \textrm{Dir}(\alpha/K) \\
          \Theta & := & \sum_{k=1}^K \pi_k \delta_{\mu_k} \\
          (\theta_n)_{n=1\ldots N} & \simiid & \Theta \\
          (x_i)_{n=1\ldots N} & \sim & \mathcal{N}(\theta_n, 1)
        \end{eqnarray*}
      \end{block}
    \end{column}
    \begin{column}{.47\textwidth}
      \begin{block}{(Pseudo) MATLAB code}
        \vspace{0.75\baselineskip}
        \begin{alltt}
          mu = randn(K,1)

          pi = dirichlet(K, alpha/K)

          for n = 1:N
            
          \ \ theta = mu(mnrnd(1,pi))
          
          \ \ x(n) \ = theta + randn
            
          end
        \end{alltt}
        \vspace{0.75\baselineskip}
      \end{block}
    \end{column}
  \end{columns}
  \vspace{\baselineskip}
  \begin{center}
  {
    {Note that the pseudo-code makes the potentially ambiguous\\conditional independence structure transparent}
  }	
  \end{center}
  \vspace{-1\baselineskip}
\end{frame}

\begin{frame}{Example: Infinite mixture of Gaussians}
  \begin{block}{Change to generative model}
    \begin{eqnarray*}
      (\mu_i)_{i=1\ldots k} \simiid \mathcal{N}(0, 1) &\\
      (\pi_i)_{i=1\ldots k} \sim \textrm{Dir}(\alpha/k) & \\
      \Theta := \sum_{i=1}^k \pi_i \delta_{\mu_i} & \underset{k \to \infty}{\to} & \Theta \sim \textrm{DP}(\alpha, \mathcal{N}(0,1))
    \end{eqnarray*}
  \end{block}
  \begin{block}{Avoiding infinity}
    \vspace{0.5\baselineskip}
    \begin{itemize}
      \item $\Theta$ is now infinitely complex, and can only be represented approximately with finite resources
      \vspace{0.5\baselineskip}
      \item However, we can sample a finite number of samples $(\theta_i)_{i=1\ldots n}$ from some unknown $\Theta$ in finite time (with probability 1) using a stick breaking algorithm
    \end{itemize}
  \end{block}
\end{frame}  

\begin{frame}{Example: Infinite mixture of Gaussians}
  \begin{block}{(Pseudo) MATLAB code - stick breaking construction}
    \begin{alltt}
      sticks = [];
      
      atoms = [];
      
      
      for i = 1:n
      
      \ \ p = rand;
      
      \ \ while p > sum(sticks)
      
      \ \ \ \ sticks(end+1) = (1-sum(sticks)) * betarnd(1, alpha);
      
      \ \ \ \ atoms(end+1) \ = randn;
      
      \ \ end
      
      \ \ theta(i) = atoms(find(cumsum(sticks)>=p, 1, `first'));
      
      end
      
      
      x = theta' + randn(n, 1);
    \end{alltt}
  \end{block}
\end{frame}

\begin{frame}{Stochastic memoisation}
  \TBD{Probably delete unless I can find a nice way to say this}
  \begin{itemize}
    \item Stick breaking considered useful enough to be a language construction \TBD{Correct language?} in Church
    \vspace{\baselineskip}
    \item A random parameterless function $P$ can be stochastically memoised
    \begin{itemize}
      \item Mathematically equivalent to $P \to \textrm{DP}(\alpha, P)$
    \end{itemize}
    \vspace{\baselineskip}
    \item Called stochastic memoisation since the base function $P$ is called only occasionally with old return values being recycled on some calls
    \vspace{\baselineskip}
    \item Implemented via stick breaking \TBD{Is this true?}
    \vspace{\baselineskip}
    \item Latest descendant of Church (Venture) has a CRP construction which allows DP like behaviour when used in conjunction with normal memoisation
  \end{itemize}
\end{frame}

\begin{frame}{Demos}

\end{frame}


\begin{frame}{Compiler Development}
  \begin{itemize}
    \item 1950s: Ask a programmer to implement an algorithm efficiently:  They'll write it on their own in assembly.
    \begin{itemize}
      \item No good compilers; problem-dependent optimizations that only human expert could see.
    \end{itemize}
    \vspace{\baselineskip}
    \item 1970s: Novice programmers use high-level languages and let compiler work out details, experts still write assembly.
    \begin{itemize}
      \item Experts still write custom assembly when speed critical.
    \end{itemize}
    \vspace{\baselineskip}
    \item 2000s: On most problems, even experts can't write faster assembly than optimizing compilers.
    \begin{itemize}
      \item can automatically profile (JIT).
      \item can take advantage of paralellization, complicated hardware, make appropriate choices w.r.t. caching.
      \item Compilers embody decades of compiler research
    \end{itemize}    
    \vspace{\baselineskip}
  \end{itemize}
\end{frame}


\begin{frame}{Inference Methods in the Future}
  \begin{itemize}
    \item 2010: Ask a grad student to implement inference efficiently:  They'll write it on their own.
    \begin{itemize}
      \item No good automatic inference engines; problem-dependent optimizations that only human expert can see.
    \end{itemize}
    \vspace{\baselineskip}
    \item 2015: Novice grad students use automatic inference engines and let compiler work out details, experts still write their own inference.
    \begin{itemize}
      \item Experts still write custom inference when speed critical.
    \end{itemize}
    \vspace{\baselineskip}
    \item 2020: On most problems, even experts can't write faster inference than mature automatic inference engines.
    \begin{itemize}
      \item Can use paralellization, sophisticated hardware
      \item Can automatically choose appropriate methods (meta-reasoning?).
      \item Inference engines will embody 1 decade (!) of PP research.
    \end{itemize}    
    \vspace{\baselineskip}
  \end{itemize}
\end{frame}


\begin{frame}{How to write a Bayesian modeling paper: 2010}
  \begin{block}{}
    \begin{enumerate}
      \item Write down a generative model in an afternoon
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Get 2 grad students to implement inference for a month
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Use technical details of inference to pad half of the paper
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}{How to write a Bayesian modeling paper: 2020}
  \begin{block}{}
    \begin{enumerate}
      \item Write down a generative model in an afternoon
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item \sout{Get 2 grad students to implement inference for a month} 
      \\ Run automatic inference engine.
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Use technical details of inference to pad half of the paper
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}{How to write a Bayesian modeling paper: 2020}
  \begin{block}{}
    \begin{enumerate}
      \item Write down a generative model in an afternoon
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item \sout{Get 2 grad students to implement inference for a month} 
      \\Run automatic inference engine.
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item \sout{Use technical details of inference to pad half of the paper}
      \\ Discuss strengths and weaknesses of the model in the extra 4 pages.
    \end{enumerate}
  \end{block}
\end{frame}


\end{document}


