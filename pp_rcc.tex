\input{header_beamer}

\usecolortheme{default}
% colours
\xdefinecolor{Black}{rgb}{0,0,0}
\xdefinecolor{White}{rgb}{1,1,1}
\xdefinecolor{DarkBlue}{rgb}{0,0,.7}
\xdefinecolor{DarkRed}{rgb}{.7,0,0}
\xdefinecolor{Red}{rgb}{.85,0,0}
\xdefinecolor{DarkGreen}{rgb}{0,.7,0}
\xdefinecolor{DarkMagenta}{rgb}{.6,0,.6}
\def\Black{\textcolor{Black}}
\def\White{\textcolor{White}}
\def\Blue{\textcolor{DarkBlue}}
\def\Magenta{\textcolor{DarkMagenta}}
\def\Red{\textcolor{Red}}
\def\Green{\textcolor{DarkGreen}}

\usepackage{alltt}
\usepackage{psfrag}
\usepackage{pstool}

\usepackage{picins}
\usepackage{tikz}
\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes,calc}
\tikzstyle{mybox} = [draw=white, rectangle]

\title[] % (optional, use only with long paper titles)
{Introduction to probabilistic programming}

\author % (optional, use only with lots of authors)
{David Duvenaud and James Lloyd}
% - Use the \inst{?} command only if the authors have different
%   affiliation.

\institute[] % (optional, but mostly needed)
{University of Cambridge}
% - Use the \inst command only if there are several affiliations.
% - Keep it simple, no one is interested in your street address.

\date % (optional)
{\empty}

\subject{Talks}

\usetikzlibrary{shapes.geometric,arrows,chains,matrix,positioning,scopes}
 \makeatletter
 \tikzset{join/.code=\tikzset{after node path={%
       \ifx\tikzchainprevious\pgfutil@empty\else(\tikzchainprevious)%
       edge[every join]#1(\tikzchaincurrent)\fi}}
 }
 \tikzset{>=stealth',every on chain/.append style={join},
   every join/.style={->}
 }

\tikzstyle{mybox} = [draw=white, rectangle]
\usepackage{ifthen}
\usepackage{booktabs}

% Custom definitions
\def\simiid{\sim_{\mbox{\tiny iid}}}

\input{commenting.tex}

%% For submission, make all render blank.
%\renewcommand{\LATER}[1]{}
%\renewcommand{\fLATER}[1]{}
%\renewcommand{\TBD}[1]{}
%\renewcommand{\fTBD}[1]{}
%\renewcommand{\PROBLEM}[1]{}
%\renewcommand{\fPROBLEM}[1]{}
%\renewcommand{\NA}[1]{#1}  %% Note, NA's pass through!

\begin{document}

\small

%% { 
%%   \setbeamertemplate{footline}{\empty}
%%   \begin{frame}
%%     \titlepage
%%   \end{frame}
%% }
\renewcommand{\inserttotalframenumber}{11}

\input{defs}

\begin{frame}
  \begin{block}{}
    \titlepage
  \end{block}
  \begin{center}
    {\bf Thanks to}\\
    Daniel M Roy (Cambridge)\\
    Roger Grosse (MIT)
  \end{center}
\end{frame}

\begin{frame}{How to write a Bayesian modeling paper}
  \begin{block}{}
    \begin{enumerate}
      \item Write down a generative model in an afternoon
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Get 2 grad students to implement inference for a month
      \vspace{\baselineskip}
      \vspace{\baselineskip}
      \item Use technical details of inference to pad half of the paper
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}{Can we do better?}
  \begin{block}{Example: Graphical Models}  
  \end{block}
      \begin{block}{Application Papers}
      \begin{enumerate}
        \item Write down a graphical model
        \item Perform inference using general-purpose software
        \item Apply to some new problem
      \end{enumerate}
    \end{block}
      \begin{block}{Inference papers}
      \begin{enumerate}
        \item Identify common structures in graphical models (e.g. chains)
        \item Develop efficient inference method
        \item Implement in a general-purpose software package
      \end{enumerate}
    \end{block}  
  \begin{block}{}
      \vspace{-2\baselineskip}
  \large
    \begin{center}
    {
      {Modeling and inference have been disentangled}
    }	
    \end{center}
  \end{block}
\end{frame}



\begin{frame}{Expressivity}
  \begin{block}{Not all models are graphical models}  
  What is the largest class of models available?
  \end{block}
  \vspace{\baselineskip}
  \begin{block}{Probabilistic Programs}
      \begin{enumerate}
        \item A probabilistic program (PP) is any program that can depend on random choices.  Can be written in any language that has a (P)RNG.
        \item You can specify any (computable) prior by simply writing down a PP that generates samples
        \item Any PP implicitly defines a distribution over execution traces
      \end{enumerate}
    \end{block}
\end{frame}

\begin{frame}{An Example Probabilistic Program}
\TBD{DD}
 % \begin{block}{}
    \begin{alltt}
    function x = simplemix()
    
    \
    
	flip = rand < 0.5;

	if flip

	\ \ x = randg + 2;   \% Random draw from Gamma(1,1)
    
	else

	\ \ x = randn;    \ \ \ \ \% Random draw from standard Normal
    
	end

    \end{alltt}
%  \end{block}
  \TBD{Call attention to inelegance (and perhaps lack of generic inference schemes) as a graphical model}
  \TBD{Maybe add a unboundedly recursive example as well}
	\pause
  \begin{block}{Implied distributions over variables}

  %\begin{figure}
  \only<1-2>{
	  \includegraphics[width=0.45\textwidth]{figures/xmarg}
	  \includegraphics[width=0.45\textwidth]{figures/flipmarg}}
  \only<3>{
	  \includegraphics[width=0.45\textwidth]{figures/joint}
	  \includegraphics[width=0.45\textwidth]{figures/flipcond}}	  
%	  \includegraphics[width=0.4\textwidth]{figures/joint}
	 % \end{figure}
  \end{block}
\end{frame}

\begin{frame}{Probabilistic Programs vs Probabilistic Programming}
\TBD{DD}
\TBD{Move this between earlier slides if sensible}
  \begin{block}{Once we've defined a prior, what do we want to do with it?}  
  The PP defines $P(D,N,H)$, we choose D to be the subset of variables we observe, H the set of variables we're interested in, and N the set of variables that we're not interested in, so we'll integrate them out.  We want to get to $P(H|D)$
  \end{block}
      \begin{block}{Probabilistic Programming}
      \begin{enumerate}
        \item Usually refers to doing inference when a PP specifies your prior.
        \item 
      \end{enumerate}
    \end{block}
    TODO: Show the two possibilities of conditioning in th eprevious program
\end{frame}

\begin{frame}{Can we develop generic inference for all PPs?}
  \begin{block}{Rejection sampling}
    \begin{enumerate}
      \item Run the program with a fresh source of random numbers
      \item If condition $D$ is true, record $H$ as a sample, else ignore the sample
      \item Repetere infinitum
    \end{enumerate}
  \end{block}
  \begin{block}{Example}
    %\vspace{0.5\baselineskip}
    %\centering
    %\input{figures/rejection-trace.tex}
    \begin{columns}
      \begin{column}{.4\textwidth}
        \input{include/simple-mix.tex}
      \end{column}
      \begin{column}{.4\textwidth}
        \input{include/rejection-trace.tex}
      \end{column}
    \end{columns}
  \end{block}
  \begin{block}{This produces samples over the \emph{execution trace}}
    \eg 
    \onslide<3->{\texttt{(True, 2.7)},}
    \onslide<9->{\texttt{(True, 2.1)},}
    \onslide<15->{\texttt{(False, 2.3)}, \dots}
  \end{block}
\end{frame}

\begin{frame}{Can we be more efficient?}
  \begin{block}{MCMC}
    \input{include/mcmc-trace.tex}
  \end{block}
\end{frame}

\begin{frame}{Can we be more efficient?}
  \begin{block}{MCMC}
    \input{include/mcmc-trace-2.tex}
  \end{block}
\end{frame}

\begin{frame}{PP via MCMC - notation}
  \begin{block}{}
    Following Wingate et alia \TBD{ref} we represent an unconditioned PP as a parameterless random function $f$ (or as a deterministic function which takes a source of randomness as an input)
    \newline
    
    Evaluating $f$ results in random choices which are denoted as
    \begin{equation*}
      x_k = f_{k|x_1,\ldots,x_{k-1}} \sim p_{t_k}(.|\theta_{k},x_1,\ldots,x_{k-1}).
    \end{equation*}
    where $\theta_{k}$ represents parameters of the distribution not included in $(x_1,\ldots,x_{k-1})$.
    \newline
    
    The density / probability of a particular evaluation is then
    \begin{equation*}
      p(x_1,\ldots,x_K) = \prod_{k=1}^K p_{t_k}(x_k|\theta_{k},x_1,\ldots,x_{k-1}).
    \end{equation*}
    
    We then perform MCMC over the the execution trace $x = (x_1,\ldots,x_K)$.
    
  \end{block}
\end{frame}

\begin{frame}{MCMC over execution traces}
  \begin{enumerate}
    \item Select a random decision in the execution trace $x$
    \begin{itemize}
      \item{\eg $x_k = f_k$}
    \end{itemize}
    \vspace{\baselineskip}
    \item Propose a new value
    \begin{itemize}
      \item{\eg $x_k' \sim K_{t_k}(.|x_k,\theta_k)$}
    \end{itemize}
    \vspace{\baselineskip}
    \item Run the program to determine all subsequent choices $(x_l' : l > k)$, reusing current choices where possible
    \vspace{\baselineskip}
    \item Propose moving from the state $(x_1,\ldots,x_K)$ to $(x_1,\ldots,x_{k-1},x_k',\ldots,x_{K'}')$
    \vspace{\baselineskip}
    \item Accept the change with the appropriate reversible jump MCMC acceptance probability
  \end{enumerate}
  \begin{equation*}
    \frac{K_{t_k}(x_k|x_k',\theta_k)\prod_{i=k}^{K'} p_{t_i'}(x_i'|\theta_{i}',x_1,\ldots,x_{k-1},x_k',\ldots,x_{i-1}')}{K_{t_k}(x_k'|x_k,\theta_k)\prod_{i=k}^K p_{t_i}(x_i|\theta_{i},x_1,\ldots,x_{i-1})}
  \end{equation*}
\end{frame}

%\begin{frame}{Worked example}
%\TBD{If there is time}
%\end{frame}

\begin{frame}{Further generic inference methods}
\TBD{Dave}
Now that we have separated inference and model design, what inference methods could we do

\eg HMC, parallel tempering, etc. (with citations)

Remember graphical models (fancy algorithms that work in certain model classes)
\end{frame}

\begin{frame}{Main strength}
\TBD{Dave}
Writing non-parametric models easy

Only need to evaluate samples from non-parametric objext lazily
\end{frame}

\begin{frame}{Nonparametric models}
  \begin{itemize}
    \item If we can sample from the prior of a nonparametric model using finite resources (with probability 1), then we can apply probabilistic programming.
    \vspace{\baselineskip}
    \item This can be achieved for a number of nonparametric processes/models \eg
    \begin{itemize}
      \item Marginalisation (\eg Gaussian process)
      \item Stick breaking (\eg Dirichlet process \TBD{Dan, you ok with this language?})
      \item Urn scheme (\eg Indian Buffet process)\dots
    \end{itemize}
    \vspace{\baselineskip}
    \item Current research to produce finite sampling algorithms for other nonparametric processes (\eg negative binomial process \TBD{cite Koa and Dan})
  \end{itemize}
\end{frame}

\begin{frame}{Example: Mixture of Gaussians}
  \begin{columns}
    \begin{column}{.459\textwidth}
      \begin{block}{Generative model}
        \begin{eqnarray*}
          (\mu_i)_{i=1\ldots k} & \simiid & \mathcal{N}(0, 1) \\
          (\pi_i)_{i=1\ldots k} & \sim & \textrm{Dir}(\alpha) \\
          \Theta & := & \sum_{i=1}^k \pi_i \delta_{\mu_i} \\
          (\theta_i)_{i=1\ldots n} & \simiid & \Theta \\
          (x_i)_{i=1\ldots n} & \simiid & \mathcal{N}(\theta_i, 1)
        \end{eqnarray*}
      \end{block}
    \end{column}
    \begin{column}{.459\textwidth}
      \begin{block}{(Pseudo) MATLAB code}
        \vspace{0.75\baselineskip}
        \begin{alltt}
          mu = randn(k,1)

          pi = dirichlet(k, alpha)


          for i = 1:n
            
          \ \ theta = mu(mnrnd(1,pi))
          
          \ \ x(i) \ = theta + randn
            
          end
        \end{alltt}
        \vspace{0.75\baselineskip}
      \end{block}
    \end{column}
  \end{columns}
\end{frame}

\begin{frame}{Example: Infinite mixture of Gaussians}
  \begin{block}{Change to generative model}
    \begin{equation*}
      \Theta := \sum_{i=1}^k \pi_i \delta_{\mu_i} \to \Theta \sim \textrm{DP}(\alpha, \mathcal{N}(0,1))
    \end{equation*}
  \end{block}
  \begin{block}{(Pseudo) MATLAB code - stick breaking construction}
    \begin{alltt}
      sticks = []; atoms = [];
      
      for i = 1:n
      
      \ \ p = rand;
      
      \ \ while p > sum(sticks)
      
      \ \ \ \ sticks(end+1) = (1-sum(sticks)) * betarnd(1, alpha);
      
      \ \ \ \ atoms(end+1) \ = randn;
      
      \ \ end
      
      \ \ theta(i) = atoms(find(cumsum(sticks)>=p, 1, `first'));
      
      end

      x = theta' + randn(n, 1);
    \end{alltt}
  \end{block}
\end{frame}

\begin{frame}{Stochastic memoisation}
  \begin{itemize}
    \item Stick breaking considered useful enough to be a language construction in Church
    \vspace{\baselineskip}
    \item A probabilistic function $P$ can be stochastically memoised
    \begin{itemize}
      \item Mathematically equivalent to $P \to \textrm{DP}(\alpha, P)$
    \end{itemize}
    \vspace{\baselineskip}
    \item Called stochastic memoisation since the new function would return the same value proababilistically, consisted with Dirichlet process based model
    \vspace{\baselineskip}
    \item Implemented via stick breaking
  \end{itemize}
\end{frame}

\begin{frame}{PP timeline / videos / whatever}

Infer.net?
\end{frame}

\end{document}


